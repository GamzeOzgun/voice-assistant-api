from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

# AÃ§Ä±k eriÅŸimli ve stabil model
model_name = "EleutherAI/gpt-neo-1.3B"

# Tokenizer ve model yÃ¼kleniyor
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# Text generation pipeline kuruluyor
generator = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    device=0  # CPU kullanÄ±mÄ±
)

def generate_answer(prompt: str) -> str:
    prompt_lower = prompt.lower().strip()

    # ğŸ”’ Kritik sorulara sabit yanÄ±t
    if "sipariÅŸ" in prompt_lower or "sipariÅŸim" in prompt_lower:
        return "SipariÅŸiniz 2-3 gÃ¼n iÃ§erisinde elinize ulaÅŸacaktÄ±r."

    # ğŸ”“ DiÄŸer sorular iÃ§in LLM ile Ã¼retim
    safe_prompt = f"Soru: {prompt.strip()}\nCevap:"
    result = generator(
        safe_prompt,
        max_new_tokens=100,
        do_sample=True,
        temperature=0.7,
        top_p=0.95
    )[0]["generated_text"]

    # YanÄ±tÄ± sadece 'Cevap:' kÄ±smÄ±ndan sonrasÄ±nÄ± al
    answer = result.split("Cevap:")[-1].strip()

    # EÄŸer model 'Soru:' kÄ±smÄ±nÄ± tekrar ediyorsa, onu temizle
    if "Soru:" in answer:
        answer = answer.split("Soru:")[0].strip()

    return answer
